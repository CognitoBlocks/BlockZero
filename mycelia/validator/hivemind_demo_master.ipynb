{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8e977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share this with other peers: ['/ip4/127.0.0.1/tcp/35493/p2p/12D3KooWFDdqSPnGkDfnt3po3AVemkeunY8NkP8mjDNYtTRiXE2d']\n"
     ]
    }
   ],
   "source": [
    "# allreduce_tensor.py\n",
    "import torch, time\n",
    "import hivemind\n",
    "\n",
    "# 1) Start or join the DHT (peer discovery/coordination)\n",
    "#    First peer: dht = hivemind.DHT(start=True)\n",
    "#    Other peers: put the printed address from the first peer into initial_peers=[\"...\"]\n",
    "dht = hivemind.DHT(start=True, client_mode = False)\n",
    "print(\"Share this with other peers:\", [str(a) for a in dht.get_visible_maddrs()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3adb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sn owner:  ['/ip4/127.0.0.1/tcp/46065/p2p/12D3KooWBFxNgqoaYwdeF7v1gtkFnwRX8n2UqRSPLQJkEGHesE2N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcfbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local before: [5.0, 5.0, 5.0, 5.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-15' coro=<DecentralizedAverager._declare_for_download_periodically() done, defined at /home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/averager.py:600> exception=RuntimeError('Broken pipe')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/averager.py\", line 609, in _declare_for_download_periodically\n",
      "    self.dht.store(\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/dht/dht.py\", line 212, in store\n",
      "    future = MPFuture()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 93, in __init__\n",
      "    self._shared_state_code = SharedBytes.next()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 52, in next\n",
      "    cls._buffer = torch.empty([buffer_size], dtype=torch.uint8).share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 839, in share_memory_\n",
      "    self._typed_storage()._share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 1195, in _share_memory_\n",
      "    self._untyped_storage.share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 522, in share_memory_\n",
      "    return super().share_memory_(*args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 400, in share_memory_\n",
      "    self._share_filename_cpu_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 530, in _share_filename_cpu_\n",
      "    return super()._share_filename_cpu_(*args, **kwargs)\n",
      "RuntimeError: Broken pipe\n",
      "Oct 29 04:07:03.544 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.utils.asyncio.await_cancelled:89\u001b[0m] Exception in <Task finished name='Task-23504' coro=<PotentialLeaders._update_queue_periodically() done, defined at /home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/matchmaking.py:500> exception=RuntimeError('Broken pipe')>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/asyncio.py\", line 84, in await_cancelled\n",
      "    await awaitable\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/matchmaking.py\", line 503, in _update_queue_periodically\n",
      "    new_peers = await key_manager.get_averagers(key_manager.current_key, only_active=True)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/key_manager.py\", line 80, in get_averagers\n",
      "    result = await self.dht.get(group_key, latest=True, return_future=True)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/dht/dht.py\", line 178, in get\n",
      "    future = MPFuture()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 93, in __init__\n",
      "    self._shared_state_code = SharedBytes.next()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 52, in next\n",
      "    cls._buffer = torch.empty([buffer_size], dtype=torch.uint8).share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 839, in share_memory_\n",
      "    self._typed_storage()._share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 1195, in _share_memory_\n",
      "    self._untyped_storage.share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 522, in share_memory_\n",
      "    return super().share_memory_(*args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 400, in share_memory_\n",
      "    self._share_filename_cpu_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 530, in _share_filename_cpu_\n",
      "    return super()._share_filename_cpu_(*args, **kwargs)\n",
      "RuntimeError: Broken pipe\n",
      "Oct 29 04:07:03.546 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.utils.asyncio.await_cancelled:89\u001b[0m] Exception in <Task finished name='Task-23505' coro=<PotentialLeaders._declare_averager_periodically() done, defined at /home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/matchmaking.py:524> exception=RuntimeError('Broken pipe')>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/matchmaking.py\", line 535, in _declare_averager_periodically\n",
      "    await key_manager.declare_averager(group_key, self.peer_id, expiration_time=new_expiration_time)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/key_manager.py\", line 62, in declare_averager\n",
      "    return await self.dht.store(\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/dht/dht.py\", line 212, in store\n",
      "    future = MPFuture()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 93, in __init__\n",
      "    self._shared_state_code = SharedBytes.next()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 52, in next\n",
      "    cls._buffer = torch.empty([buffer_size], dtype=torch.uint8).share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 839, in share_memory_\n",
      "    self._typed_storage()._share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 1195, in _share_memory_\n",
      "    self._untyped_storage.share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 522, in share_memory_\n",
      "    return super().share_memory_(*args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 400, in share_memory_\n",
      "    self._share_filename_cpu_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 530, in _share_filename_cpu_\n",
      "    return super()._share_filename_cpu_(*args, **kwargs)\n",
      "RuntimeError: Broken pipe\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/asyncio.py\", line 84, in await_cancelled\n",
      "    await awaitable\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/matchmaking.py\", line 544, in _declare_averager_periodically\n",
      "    await key_manager.declare_averager(\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/key_manager.py\", line 62, in declare_averager\n",
      "    return await self.dht.store(\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/dht/dht.py\", line 212, in store\n",
      "    future = MPFuture()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 93, in __init__\n",
      "    self._shared_state_code = SharedBytes.next()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py\", line 52, in next\n",
      "    cls._buffer = torch.empty([buffer_size], dtype=torch.uint8).share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 839, in share_memory_\n",
      "    self._typed_storage()._share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 1195, in _share_memory_\n",
      "    self._untyped_storage.share_memory_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 522, in share_memory_\n",
      "    return super().share_memory_(*args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 400, in share_memory_\n",
      "    self._share_filename_cpu_()\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 451, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/torch/storage.py\", line 530, in _share_filename_cpu_\n",
      "    return super()._share_filename_cpu_(*args, **kwargs)\n",
      "RuntimeError: Broken pipe\n",
      "Oct 29 04:07:03.548 [\u001b[1m\u001b[31mERROR\u001b[0m] [\u001b[1mhivemind.averaging.averager._step:481\u001b[0m] Averaging step failed: could not find a group\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/averager.py\", line 451, in _step\n",
      "    raise AllreduceException(\"Averaging step failed: could not find a group\")\n",
      "hivemind.averaging.partition.AllreduceException: Averaging step failed: could not find a group\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Make a tensor you'd like to all-reduce (sum/average)\n",
    "local = torch.ones(4) * (torch.randint(1, 10, ()).item())  # e.g., [k, k, k, k]\n",
    "print(\"local before:\", local.tolist())\n",
    "\n",
    "# 3) Create an averager for that tensor; all peers must use the SAME prefix\n",
    "averager = hivemind.averaging.DecentralizedAverager(\n",
    "    averaged_tensors=[local], dht=dht, start=True, prefix=\"demo/allreduce\", target_group_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0905c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local before: [36.5, 36.5, 36.5, 36.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal before:\u001b[39m\u001b[38;5;124m\"\u001b[39m, local\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 4) Run one all-reduce round (blocks until a group forms or times out)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#    By default, it computes the *average* in-place; set averaging_alpha=1 to overwrite with the average.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#    You can pass weight=<float> to do a weighted average.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43maverager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup info:\u001b[39m\u001b[38;5;124m\"\u001b[39m, info)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# After step(), `local` now holds the averaged values from all peers in the group.\u001b[39;00m\n",
      "File \u001b[0;32m~/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/averaging/averager.py:419\u001b[0m, in \u001b[0;36mDecentralizedAverager.step\u001b[0;34m(self, gather, scheduled_time, weight, timeout, allow_retries, require_trigger, wait)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m require_trigger:\n\u001b[1;32m    418\u001b[0m     step\u001b[38;5;241m.\u001b[39mallow_allreduce()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m wait \u001b[38;5;28;01melse\u001b[39;00m step\n",
      "File \u001b[0;32m~/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/hivemind/utils/mpfuture.py:254\u001b[0m, in \u001b[0;36mMPFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetpid() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin_pid:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly the process that created MPFuture can await result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m base\u001b[38;5;241m.\u001b[39mCANCELLED:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m base\u001b[38;5;241m.\u001b[39mCancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "local += torch.ones(4) * (torch.randint(1, 10, ()).item())\n",
    "print(\"local before:\", local.tolist())\n",
    "\n",
    "# 4) Run one all-reduce round (blocks until a group forms or times out)\n",
    "#    By default, it computes the *average* in-place; set averaging_alpha=1 to overwrite with the average.\n",
    "#    You can pass weight=<float> to do a weighted average.\n",
    "info = averager.step(timeout=30.0, gather = {'step': 21})\n",
    "print(\"group info:\", info)\n",
    "\n",
    "# After step(), `local` now holds the averaged values from all peers in the group.\n",
    "print(\"local after:\", local.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72dc0d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<libp2p.peer.id.ID (12D3KooWFWDyCB2qF2QDjgQQz4KCxwxhpYtZppHjMMSkTHtGyfRY)>: None,\n",
       " <libp2p.peer.id.ID (12D3KooWKgseFFwQJ5rRTh17vfpGomZavLcQYtbxgMmgbXqEf2DN)>: None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "averager.shutdown()\n",
    "dht.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a441ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mycelia.config import MinerConfig, ValidatorConfig, parse_args\n",
    "from mycelia.shared.model import load_base_model \n",
    "\n",
    "rank = 0\n",
    "config = ValidatorConfig() \n",
    "model, em = load_base_model(rank, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5144147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_orig_mod.model.embed_tokens.weight',\n",
       " '_orig_mod.model.layers.0.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.0.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.0.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.0.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.0.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.0.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.0.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.0.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.0.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.0.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.0.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.0.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.1.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.1.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.1.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.1.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.1.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.1.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.1.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.0.gate_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.0.up_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.0.down_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.1.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.1.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.1.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.1.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.2.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.2.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.2.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.2.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.2.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.2.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.2.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.2.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.2.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.2.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.2.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.2.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.3.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.3.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.3.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.3.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.3.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.3.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.3.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.7.gate_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.7.up_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.7.down_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.3.gate_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.3.up_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.3.down_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.3.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.3.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.3.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.3.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.4.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.4.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.4.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.4.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.4.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.4.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.4.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.4.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.4.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.4.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.4.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.4.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.5.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.5.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.5.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.5.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.5.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.5.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.5.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.3.gate_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.3.up_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.3.down_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.7.gate_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.7.up_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.7.down_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.5.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.5.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.5.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.5.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.6.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.6.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.6.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.6.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.6.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.6.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.6.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.6.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.6.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.6.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.6.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.6.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.7.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.7.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.7.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.7.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.7.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.7.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.7.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.0.gate_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.0.up_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.0.down_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.7.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.7.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.7.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.7.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.8.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.8.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.8.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.8.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.8.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.8.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.8.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.8.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.8.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.8.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.8.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.8.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.9.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.9.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.9.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.9.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.9.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.9.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.9.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.3.gate_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.3.up_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.3.down_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.1.gate_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.1.up_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.1.down_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.0.gate_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.0.up_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.0.down_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.9.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.9.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.9.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.9.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.10.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.10.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.10.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.10.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.10.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.10.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.10.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.10.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.10.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.10.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.10.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.10.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.11.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.11.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.11.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.11.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.11.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.11.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.11.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.0.gate_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.0.up_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.0.down_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.11.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.11.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.11.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.11.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.12.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.12.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.12.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.12.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.12.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.12.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.12.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.12.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.12.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.12.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.12.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.12.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.13.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.13.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.13.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.13.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.13.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.13.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.13.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.7.gate_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.7.up_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.7.down_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.13.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.13.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.13.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.13.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.14.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.14.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.14.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.14.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.14.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.14.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.14.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.14.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.14.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.14.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.14.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.14.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.15.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.15.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.15.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.15.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.15.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.15.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.15.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.1.gate_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.1.up_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.1.down_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.15.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.15.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.15.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.15.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.16.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.16.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.16.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.16.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.16.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.16.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.16.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.16.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.16.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.16.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.16.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.16.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.17.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.17.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.17.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.17.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.17.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.17.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.17.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.7.gate_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.7.up_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.7.down_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.17.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.17.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.17.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.17.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.18.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.18.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.18.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.18.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.18.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.18.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.18.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.18.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.18.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.18.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.18.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.18.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.19.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.19.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.19.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.19.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.19.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.19.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.19.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.7.gate_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.7.up_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.7.down_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.19.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.19.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.19.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.19.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.20.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.20.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.20.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.20.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.20.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.20.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.20.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.20.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.20.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.20.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.20.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.20.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.21.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.21.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.21.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.21.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.21.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.21.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.21.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.7.gate_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.7.up_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.7.down_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.0.gate_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.0.up_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.0.down_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.5.gate_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.5.up_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.5.down_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.4.gate_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.4.up_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.experts.4.down_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.21.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.21.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.21.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.21.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.22.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.22.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.22.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.22.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.22.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.22.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.22.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.22.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.22.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.22.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.22.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.22.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.23.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.23.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.23.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.23.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.23.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.23.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.23.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.1.gate_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.1.up_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.1.down_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.3.gate_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.3.up_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.3.down_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.23.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.23.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.23.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.23.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.24.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.24.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.24.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.24.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.24.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.24.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.24.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.24.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.24.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.24.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.24.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.24.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.25.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.25.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.25.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.25.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.25.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.25.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.25.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.3.gate_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.3.up_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.3.down_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.1.gate_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.1.up_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.experts.1.down_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.25.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.25.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.25.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.25.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.26.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.26.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.26.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.26.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.26.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.26.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.26.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.26.mlp.gate_proj.weight',\n",
       " '_orig_mod.model.layers.26.mlp.up_proj.weight',\n",
       " '_orig_mod.model.layers.26.mlp.down_proj.weight',\n",
       " '_orig_mod.model.layers.26.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.26.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.layers.27.self_attn.q_a_proj.weight',\n",
       " '_orig_mod.model.layers.27.self_attn.q_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.27.self_attn.q_b_proj.weight',\n",
       " '_orig_mod.model.layers.27.self_attn.kv_a_proj_with_mqa.weight',\n",
       " '_orig_mod.model.layers.27.self_attn.kv_a_layernorm.weight',\n",
       " '_orig_mod.model.layers.27.self_attn.kv_b_proj.weight',\n",
       " '_orig_mod.model.layers.27.self_attn.o_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.7.gate_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.7.up_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.7.down_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.2.gate_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.2.up_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.2.down_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.6.gate_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.6.up_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.6.down_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.0.gate_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.0.up_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.experts.0.down_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.gate.weight',\n",
       " '_orig_mod.model.layers.27.mlp.shared_experts.gate_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.shared_experts.up_proj.weight',\n",
       " '_orig_mod.model.layers.27.mlp.shared_experts.down_proj.weight',\n",
       " '_orig_mod.model.layers.27.input_layernorm.weight',\n",
       " '_orig_mod.model.layers.27.post_attention_layernorm.weight',\n",
       " '_orig_mod.model.norm.weight',\n",
       " '_orig_mod.lm_head.weight']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "for n, p in model.named_parameters():\n",
    "    names.append(n)\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155b6518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: ['_orig_mod.model.layers.1.mlp.experts.0.down_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.0.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.0.up_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.1.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.0.down_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.0.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.0.up_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.11.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.7.down_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.7.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.13.mlp.experts.7.up_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.1.down_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.1.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.1.up_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.15.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.7.down_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.7.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.17.mlp.experts.7.up_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.7.down_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.7.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.19.mlp.experts.7.up_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.0.down_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.0.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.0.up_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.7.down_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.7.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.21.mlp.experts.7.up_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.1.down_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.1.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.1.up_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.3.down_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.3.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.3.up_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.23.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.1.down_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.1.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.1.up_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.3.down_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.3.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.3.up_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.25.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.0.down_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.0.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.0.up_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.7.down_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.7.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.27.mlp.experts.7.up_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.3.down_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.3.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.3.up_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.7.down_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.7.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.3.mlp.experts.7.up_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.2.down_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.2.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.2.up_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.3.down_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.3.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.3.up_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.7.down_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.7.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.5.mlp.experts.7.up_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.0.down_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.0.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.0.up_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.4.down_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.4.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.4.up_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.5.down_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.5.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.5.up_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.7.mlp.experts.6.up_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.0.down_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.0.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.0.up_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.1.down_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.1.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.1.up_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.3.down_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.3.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.3.up_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.6.down_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.6.gate_proj.weight',\n",
       "  '_orig_mod.model.layers.9.mlp.experts.6.up_proj.weight']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mycelia.shared.modeling.modeling_mycelia import get_layer_expert_id\n",
    "# from mycelia.validator.inter_validator_connection import iter_named_grads\n",
    "\n",
    "all_named = list(iter_named_grads(model))\n",
    "all_named.sort(key=lambda kv: kv[0])  # deterministic order\n",
    "name_to_tensor = dict(all_named)\n",
    "\n",
    "expert_group_to_names = {group_id: [] for group_id, _ in list(em.expert_group_assignment.values())[0].items()}\n",
    "\n",
    "for name, p in name_to_tensor.items():\n",
    "    layer_id, expert_id = get_layer_expert_id(name) \n",
    "    if layer_id and expert_id is not None:\n",
    "        for group_id, expert_ids in em.expert_group_assignment[layer_id].items():\n",
    "            if expert_id in expert_ids:\n",
    "                expert_group_to_names[group_id].append(name)\n",
    "\n",
    "expert_group_to_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "910ba91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_orig_mod.model.embed_tokens.weight\n",
      "_orig_mod.model.layers.0.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.0.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.0.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.0.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.0.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.0.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.0.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.0.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.0.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.0.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.0.input_layernorm.weight\n",
      "_orig_mod.model.layers.0.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.1.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.1.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.1.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.1.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.1.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.1.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.1.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.0.gate_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.0.up_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.0.down_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.gate.weight\n",
      "_orig_mod.model.layers.1.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.1.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.1.input_layernorm.weight\n",
      "_orig_mod.model.layers.1.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.2.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.2.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.2.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.2.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.2.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.2.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.2.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.2.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.2.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.2.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.2.input_layernorm.weight\n",
      "_orig_mod.model.layers.2.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.3.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.3.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.3.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.3.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.3.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.3.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.3.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.7.gate_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.7.up_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.7.down_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.3.gate_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.3.up_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.3.down_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.gate.weight\n",
      "_orig_mod.model.layers.3.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.3.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.3.input_layernorm.weight\n",
      "_orig_mod.model.layers.3.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.4.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.4.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.4.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.4.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.4.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.4.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.4.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.4.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.4.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.4.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.4.input_layernorm.weight\n",
      "_orig_mod.model.layers.4.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.5.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.5.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.5.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.5.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.5.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.5.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.5.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.3.gate_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.3.up_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.3.down_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.7.gate_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.7.up_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.7.down_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.gate.weight\n",
      "_orig_mod.model.layers.5.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.5.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.5.input_layernorm.weight\n",
      "_orig_mod.model.layers.5.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.6.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.6.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.6.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.6.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.6.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.6.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.6.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.6.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.6.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.6.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.6.input_layernorm.weight\n",
      "_orig_mod.model.layers.6.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.7.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.7.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.7.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.7.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.7.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.7.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.7.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.0.gate_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.0.up_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.0.down_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.gate.weight\n",
      "_orig_mod.model.layers.7.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.7.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.7.input_layernorm.weight\n",
      "_orig_mod.model.layers.7.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.8.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.8.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.8.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.8.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.8.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.8.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.8.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.8.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.8.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.8.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.8.input_layernorm.weight\n",
      "_orig_mod.model.layers.8.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.9.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.9.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.9.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.9.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.9.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.9.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.9.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.3.gate_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.3.up_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.3.down_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.1.gate_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.1.up_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.1.down_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.0.gate_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.0.up_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.0.down_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.gate.weight\n",
      "_orig_mod.model.layers.9.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.9.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.9.input_layernorm.weight\n",
      "_orig_mod.model.layers.9.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.10.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.10.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.10.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.10.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.10.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.10.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.10.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.10.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.10.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.10.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.10.input_layernorm.weight\n",
      "_orig_mod.model.layers.10.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.11.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.11.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.11.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.11.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.11.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.11.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.11.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.0.gate_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.0.up_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.0.down_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.gate.weight\n",
      "_orig_mod.model.layers.11.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.11.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.11.input_layernorm.weight\n",
      "_orig_mod.model.layers.11.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.12.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.12.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.12.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.12.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.12.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.12.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.12.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.12.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.12.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.12.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.12.input_layernorm.weight\n",
      "_orig_mod.model.layers.12.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.13.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.13.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.13.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.13.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.13.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.13.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.13.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.7.gate_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.7.up_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.7.down_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.gate.weight\n",
      "_orig_mod.model.layers.13.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.13.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.13.input_layernorm.weight\n",
      "_orig_mod.model.layers.13.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.14.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.14.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.14.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.14.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.14.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.14.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.14.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.14.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.14.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.14.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.14.input_layernorm.weight\n",
      "_orig_mod.model.layers.14.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.15.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.15.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.15.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.15.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.15.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.15.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.15.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.1.gate_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.1.up_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.1.down_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.gate.weight\n",
      "_orig_mod.model.layers.15.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.15.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.15.input_layernorm.weight\n",
      "_orig_mod.model.layers.15.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.16.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.16.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.16.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.16.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.16.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.16.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.16.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.16.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.16.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.16.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.16.input_layernorm.weight\n",
      "_orig_mod.model.layers.16.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.17.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.17.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.17.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.17.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.17.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.17.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.17.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.7.gate_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.7.up_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.7.down_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.gate.weight\n",
      "_orig_mod.model.layers.17.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.17.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.17.input_layernorm.weight\n",
      "_orig_mod.model.layers.17.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.18.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.18.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.18.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.18.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.18.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.18.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.18.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.18.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.18.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.18.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.18.input_layernorm.weight\n",
      "_orig_mod.model.layers.18.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.19.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.19.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.19.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.19.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.19.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.19.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.19.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.7.gate_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.7.up_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.7.down_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.gate.weight\n",
      "_orig_mod.model.layers.19.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.19.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.19.input_layernorm.weight\n",
      "_orig_mod.model.layers.19.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.20.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.20.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.20.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.20.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.20.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.20.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.20.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.20.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.20.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.20.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.20.input_layernorm.weight\n",
      "_orig_mod.model.layers.20.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.21.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.21.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.21.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.21.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.21.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.21.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.21.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.7.gate_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.7.up_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.7.down_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.0.gate_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.0.up_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.0.down_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.5.gate_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.5.up_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.5.down_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.4.gate_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.4.up_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.experts.4.down_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.gate.weight\n",
      "_orig_mod.model.layers.21.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.21.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.21.input_layernorm.weight\n",
      "_orig_mod.model.layers.21.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.22.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.22.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.22.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.22.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.22.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.22.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.22.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.22.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.22.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.22.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.22.input_layernorm.weight\n",
      "_orig_mod.model.layers.22.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.23.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.23.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.23.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.23.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.23.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.23.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.23.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.1.gate_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.1.up_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.1.down_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.3.gate_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.3.up_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.3.down_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.gate.weight\n",
      "_orig_mod.model.layers.23.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.23.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.23.input_layernorm.weight\n",
      "_orig_mod.model.layers.23.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.24.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.24.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.24.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.24.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.24.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.24.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.24.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.24.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.24.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.24.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.24.input_layernorm.weight\n",
      "_orig_mod.model.layers.24.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.25.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.25.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.25.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.25.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.25.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.25.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.25.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.3.gate_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.3.up_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.3.down_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.1.gate_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.1.up_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.experts.1.down_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.gate.weight\n",
      "_orig_mod.model.layers.25.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.25.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.25.input_layernorm.weight\n",
      "_orig_mod.model.layers.25.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.26.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.26.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.26.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.26.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.26.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.26.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.26.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.26.mlp.gate_proj.weight\n",
      "_orig_mod.model.layers.26.mlp.up_proj.weight\n",
      "_orig_mod.model.layers.26.mlp.down_proj.weight\n",
      "_orig_mod.model.layers.26.input_layernorm.weight\n",
      "_orig_mod.model.layers.26.post_attention_layernorm.weight\n",
      "_orig_mod.model.layers.27.self_attn.q_a_proj.weight\n",
      "_orig_mod.model.layers.27.self_attn.q_a_layernorm.weight\n",
      "_orig_mod.model.layers.27.self_attn.q_b_proj.weight\n",
      "_orig_mod.model.layers.27.self_attn.kv_a_proj_with_mqa.weight\n",
      "_orig_mod.model.layers.27.self_attn.kv_a_layernorm.weight\n",
      "_orig_mod.model.layers.27.self_attn.kv_b_proj.weight\n",
      "_orig_mod.model.layers.27.self_attn.o_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.7.gate_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.7.up_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.7.down_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.2.gate_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.2.up_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.2.down_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.6.gate_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.6.up_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.6.down_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.0.gate_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.0.up_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.experts.0.down_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.gate.weight\n",
      "_orig_mod.model.layers.27.mlp.shared_experts.gate_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.shared_experts.up_proj.weight\n",
      "_orig_mod.model.layers.27.mlp.shared_experts.down_proj.weight\n",
      "_orig_mod.model.layers.27.input_layernorm.weight\n",
      "_orig_mod.model.layers.27.post_attention_layernorm.weight\n",
      "_orig_mod.model.norm.weight\n",
      "_orig_mod.lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "734064df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "def iter_named_grads(model: nn.Module, skip_none: bool = True):\n",
    "    \"\"\"\n",
    "    Yield (name, grad_tensor) for all model parameters that have gradients.\n",
    "    \"\"\"\n",
    "    for n, p in model.named_parameters():\n",
    "        # if p.grad is None:\n",
    "        #     if not skip_none:\n",
    "        #         yield n, None\n",
    "        #     continue\n",
    "        yield n, p.grad\n",
    "\n",
    "# list(iter_named_grads(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
